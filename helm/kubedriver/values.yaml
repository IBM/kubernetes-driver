global:
  # While the driver pod can run within Openshift's restricted SCC, if
  # another SCC is applied to the pod via a service account that doesn't support
  # the correct runAsUser type (i.e. MustRunAsNonRoot), arbitraryUids can be
  # disabled. Otherwise, the default value (true) shouldn't need to be updated.
  arbitraryUids: true

## Docker Image for the Openstack VIM Driver
docker:
  ## Make this the full path, including registry host and port if using one
  image: alm-docker-local.artifactory.swg-devops.com/kubedriver
  version: ${version}
  imagePullPolicy: IfNotPresent

## Configuration for the application deployment
app:
  ## Number of pods to deploy 
  replicas: 1
  ## kubedriver specific configuration
  config:
    ## Configure logging 
    log:
      level: INFO

    ## Pass additional environment variables to the application containers
    env:
      ## to support indexing of logs in Elasticsearch using Filebeat, we use logstash format. This allows
      ## us to bundle the log message and other metadata in a json log message and let Filebeat extract them
      ## as top level fields in the Elasticsearch index.
      LOG_TYPE: logstash

      ## configuration for WSGI container
      WSGI_CONTAINER: gunicorn
      ## the number of processes and threads to spawn to handle requests
      NUM_PROCESSES: "2"

    ## default_config.yml (driver configuration) overrides
    override:
      messaging:
        # Kafka connection url
        connection_address: foundation-kafka:9092
        config:
          # timeout waiting for initial version check on Kafka producer/consumer initialisation
          # 5000ms is usually sufficient, increase if problems with NoBrokersAvailable occur
          api_version_auto_timeout_ms: 5000
        topics:
          lifecycle_execution_events:
            name: lm_vnfc_lifecycle_execution_events
            auto_create: True
            num_partitions: 1
            replication_factor: 1
          job_queue:
            auto_create: True
            num_partitions: 100
            replication_factor: 1
            config:
              retention.ms: 60000
              message.timestamp.difference.max.ms: 60000
              file.delete.delay.ms: 60000

    security:
      ssl:
        enabled: True
        secret:
          ## Name of the secret containing the SSL certificate for the non-host based access (the CN of the certificate should match commonName)
          name: kubedriver-tls
          ## If True, the Helm chart installation will generate a new SSL key with a self-signed certificate
          generate: True
          ## The Common Name used for the SSL certificate (do not change this)
          commonName: kubedriver

  ## Probe configuration for checking Application availability
  livenessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 15
    periodSeconds: 30
  readinessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 25
    periodSeconds: 10
    
  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity:
    podAntiAffinity:
      ## Default anti-affinity rule is to try and schedule multiple pods of this app on different nodes
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: "kubernetes.io/hostname"
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - kubedriver
    ## Example of node affinity rule to require this pod is only scheduled on a Node with a label (key=value) of "Stateless=True"
    #nodeAffinity:
    #  requiredDuringSchedulingIgnoredDuringExecution:
    #    nodeSelectorTerms:
    #    - matchExpressions:
    #      - key: Stateless
    #        operator: In
    #        values:
    #        - "True"
    
  ## Node tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  ## Example of allowing this pod to be deployed onto a Node that has been tainted with "ReservedForLm"
  #- key: ReservedForLm
  #  operator: Exists
  #  effect: NoSchedule

  ## Configure resource requests and limits for each container of the application
  ## Default is to have no limit
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #  cpu: 250m
    #  memory: 384Mi
    # requests:
    #  cpu: 250m
    #  memory: 384Mi

  ## Autoscaler
  ## Enabling this deploys a Horizontal Pod Autoscaler that will monitor the CPU usage and scale this deployment 
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## NOTE: requires Kubernetes Metrics Server
  autoscaler:
    enabled: false
    maxReplicas: 10
    minReplicas: 1
    targetCPUUtilizationPercentage: 80

service:
  # Using NodePort allows access to the IPs through http://k8s-host:nodePort/
  type: NodePort
  nodePort: 31684

ingress:
  enabled: true
  ## The host used to access the service externally i.e. http://<server.ingress.hostname>/
  ## If not making use of a DNS to resolve this host you must add this to the client hosts file with the K8s IP
  ## E.g.
  ## 10.220.1.2     kubedriver.lm
  host: kubedriver.lm